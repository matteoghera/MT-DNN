{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "Article: *Multi-Task Deep Neural Networks for Natural Language Understanding*\n",
    "\n",
    "<a href=https://arxiv.org/abs/1901.11504> https://arxiv.org/abs/1901.11504</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Colab\n",
    "!git clone -b insertMyNotebookAndScripts https://github.com/matteoghera/MT-DNN.git\n",
    "!mkdir MT-DNN/models\n",
    "!pip install path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from path import Path\n",
    "PROJ_DIR = Path(\"/content/MT-DNN\")  #for Colab \n",
    "#PROJ_DIR = Path().getcwd().parent   #for Pycharm and AWS\n",
    "DATA_DIR = PROJ_DIR / \"data\"\n",
    "MODELS_DIR=PROJ_DIR / \"models\"\n",
    "os.chdir(PROJ_DIR)\n",
    "print(PROJ_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python myscripts/download_glue_data.py --data_dir data --tasks all  ##decommenta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -e .\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-task Deep Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from mtdnn.common.types import EncoderModelType\n",
    "from mtdnn.configuration_mtdnn import MTDNNConfig\n",
    "from mtdnn.data_builder_mtdnn import MTDNNDataBuilder\n",
    "from mtdnn.modeling_mtdnn import MTDNNModel\n",
    "from mtdnn.process_mtdnn import MTDNNDataProcess\n",
    "from mtdnn.tasks.config import MTDNNTaskDefs, MTDNNTaskConfig\n",
    "from mtdnn.tokenizer_mtdnn import MTDNNTokenizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define Configuration, Tasks and Model Objects\n",
    "ROOT_DIR = TemporaryDirectory().name\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, 'checkpoint')\n",
    "os.makedirs(OUTPUT_DIR) if not os.path.exists(OUTPUT_DIR) else OUTPUT_DIR\n",
    "\n",
    "LOG_DIR = os.path.join(ROOT_DIR, 'tensorboard_logdir')\n",
    "os.makedirs(LOG_DIR) if not os.path.exists(LOG_DIR) else LOG_DIR\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 16\n",
    "MULTI_GPU_ON = False\n",
    "MAX_SEQ_LEN = 128\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Task list\n",
    "tasks=[\"cola\", \"sst\", \"mnli\", \"rte\", \"wnli\", \"qqp\", \"mrpc\", \"snli\", \"stsb\", \"qnli\"]\n",
    "tasks=[\"rte\", \"wnli\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(OUTPUT_DIR)\n",
    "print(LOG_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = MTDNNConfig(batch_size=BATCH_SIZE, \n",
    "                     max_seq_len=MAX_SEQ_LEN, \n",
    "                     multi_gpu_on=MULTI_GPU_ON)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = MTDNNTokenizer(do_lower_case=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(str(PROJ_DIR/\"myscripts\"/\"glue_config.json\")) as f:\n",
    "  glue_config = json.load(f)\n",
    "\n",
    "my_task_config={}\n",
    "for task in tasks:\n",
    "    my_task_config[task]=glue_config[task]\n",
    "    my_task_config[task][\"data_source_dir\"]=str(DATA_DIR/my_task_config[task][\"data_paths\"][0].split(\"/\")[0])\n",
    "    \n",
    "task_defs=MTDNNTaskDefs(my_task_config)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models=[]\n",
    "for key in my_task_config.keys():\n",
    "    data_source_dir=str(DATA_DIR/my_task_config[key][\"data_paths\"][0].split(\"/\")[0])\n",
    "    data_builder = MTDNNDataBuilder(\n",
    "        tokenizer=tokenizer,\n",
    "        task_defs=task_defs,\n",
    "        data_dir=data_source_dir,\n",
    "        canonical_data_suffix=\"canonical_data\",\n",
    "        dump_rows=False,\n",
    "    )\n",
    "    vectorized_data = data_builder.vectorize()\n",
    "    \n",
    "    data_processor = MTDNNDataProcess(\n",
    "        config=config, task_defs=task_defs, vectorized_data=vectorized_data\n",
    "    )\n",
    "    \n",
    "    multitask_train_dataloader = data_processor.get_train_dataloader()\n",
    "    dev_dataloaders_list = data_processor.get_dev_dataloaders()\n",
    "    test_dataloaders_list = data_processor.get_test_dataloaders()\n",
    "    \n",
    "    decoder_opts = data_processor.get_decoder_options_list()\n",
    "    task_types = data_processor.get_task_types_list()\n",
    "    dropout_list = data_processor.get_tasks_dropout_prob_list()\n",
    "    loss_types = data_processor.get_loss_types_list()\n",
    "    kd_loss_types = data_processor.get_kd_loss_types_list()\n",
    "    tasks_nclass_list = data_processor.get_task_nclass_list()\n",
    "    test_datasets_list=[filename.split(\".\")[0] for filename in os.listdir(data_source_dir) if filename.find(\"test\")!=-1 and filename.find(\".tsv\")!=-1]\n",
    "    test_datasets_list=[filename.replace(\"test\", key) for filename in test_datasets_list]\n",
    "    \n",
    "    num_all_batches = data_processor.get_num_all_batches()\n",
    "    \n",
    "    model = MTDNNModel(\n",
    "        config,\n",
    "        task_defs,\n",
    "        pretrained_model_name=\"bert-base-uncased\",\n",
    "        num_train_step=num_all_batches,\n",
    "        decoder_opts=decoder_opts,\n",
    "        task_types=task_types,\n",
    "        dropout_list=dropout_list,\n",
    "        loss_types=loss_types,\n",
    "        kd_loss_types=kd_loss_types,\n",
    "        tasks_nclass_list=tasks_nclass_list,\n",
    "        multitask_train_dataloader=multitask_train_dataloader,\n",
    "        dev_dataloaders_list=dev_dataloaders_list,\n",
    "        test_dataloaders_list=test_dataloaders_list,\n",
    "        test_datasets_list=test_datasets_list,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        log_dir=LOG_DIR \n",
    "    )\n",
    "    models.append(model)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model in models:\n",
    "  model.fit(epochs=NUM_EPOCHS)\n",
    "  model.predict(trained_model_chckpt=f\"{OUTPUT_DIR}/model_4.pt\")\n",
    "\n",
    "  dev_result_files = list(filter(lambda x: x.endswith('.json') and 'dev' in x, os.listdir(OUTPUT_DIR))) \n",
    "  for d in dev_result_files: \n",
    "      name =  ' '.join(list(map(str.capitalize, d.split('_')))[:3]) \n",
    "      file_name = os.path.join(OUTPUT_DIR, d)\n",
    "      with open(file_name, 'r') as f: \n",
    "          res = json.load(f) \n",
    "          results.update(\n",
    "              {name: {\n",
    "                  'ACCURACY': f\"{res['metrics']['ACC']:.3f}\"\n",
    "                  }\n",
    "              }) \n",
    "          \n",
    "df_results = pd.DataFrame(results)   \n",
    "df_results\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}